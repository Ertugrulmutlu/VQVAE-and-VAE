# VQVAE-and-VAE

This repository contains a modular and reproducible pipeline to train, inspect, and evaluate a **Variational Autoencoder (VAE)** for compressing passport-style facial images.

> ğŸ’¡ This is part of a broader project comparing different image compression techniques (see blog post for VQ-VAE comparison).
---

## ğŸ§  VAE Training Pipeline

---

## ğŸ“¦ Installation

Before running any code, install the required dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ§­ Project Overview

```
VAE_Project/
â”œâ”€â”€ main.py                 # Main entry point to train the model
â”œâ”€â”€ test_main.py            # Evaluation and visual comparison on test image
â”œâ”€â”€ Utils/
â”‚   â”œâ”€â”€ model.py            # VAE architecture
â”‚   â”œâ”€â”€ train.py            # Training loop
â”‚   â”œâ”€â”€ data_preprocessor.py # Data loading and augmentation
â”‚   â”œâ”€â”€ data_inspector.py   # Dataset quality check (e.g. corrupted images)
â”œâ”€â”€ Datas/
â”‚   â”œâ”€â”€ Subclass            # Train data in here
â”‚   â”œâ”€â”€ test                # Test data in here
â”œâ”€â”€ vae_model.pt            # Trained with 20 epochs
â”œâ”€â”€ vae_model_50.pt         # Trained with 50 epochs (recommended)
â”œâ”€â”€ requirements.txt
```

> ğŸ“‚ **Model Checkpoints**:
>
> * `vae_model.pt`: trained with **20 epochs**
> * `vae_model_50.pt`: trained with **50 epochs** (recommended for better reconstruction quality)

---

## ğŸš€ How to Train

### 1. Prepare Your Dataset

Place your images inside a subfolder under `Datas/`, e.g.,

```
Datas/
â”œâ”€â”€ train/
    â””â”€â”€ your_images.jpg/png
```

Each subfolder is treated as a separate class by PyTorch's `ImageFolder`, even though labels are not used.

### 2. Run the Training Pipeline

```bash
python main.py
```

This will:

* Inspect the dataset for corrupted files (optional removal)
* Preprocess all images (resize, convert to RGB, normalize)
* Train a VAE with latent dimension 128
* Save the trained model as `vae_model_50.pt`

---

## ğŸ” Evaluate & Visualize

After training, you can evaluate and visualize the compression results:

```bash
python test_main.py
```

This script will:

* Load a test image (`Datas/test/test1.png`)
* Compress and reconstruct it using the trained model
* Save the output as JPEG (`Datas/test/reconstructed_50.jpg`)
* Print the compression ratio
* Display original vs reconstructed side-by-side

---

## ğŸ§± Code Modules

### `main.py`

Sets up the full training pipeline:

* Loads and preprocesses data
* Initializes VAE
* Trains using `Trainer`

### `test_main.py`

Standalone evaluation and visualization script to test compression results.

### `Utils/model.py`

Contains a clean PyTorch implementation of the VAE model:

* Encoder: 64x64x3 â†’ latent vector
* Decoder: latent vector â†’ 64x64x3 image

### `Utils/train.py`

Modular `Trainer` class for VAE:

* Optimizes VAE using combined MSE + KL Divergence loss
* Supports GPU/CPU

### `Utils/data_preprocessor.py`

Builds the PyTorch dataloader with:

* RGB conversion
* Resizing to 64x64
* Augmentation-ready

### `Utils/data_inspector.py`

Optional tool to check for corrupted images and summarize dataset stats.

---

## ğŸ“ Notes

* Default image resolution: **64x64 RGB**
* Latent space size: **128 dimensions**
* Output reconstructions are saved in **JPEG** with quality 85

---

## ğŸ§  VQVAE Training Pipeline

## ğŸ“¦ Installation

Before running any code, install the required dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ§­ Project Overview

```
CompressionProject/
â”œâ”€â”€ main.py                       # Entry point (VAE or VQ-VAE)
â”œâ”€â”€ test_main.py                 # Evaluation script (128x128)
â”œâ”€â”€ test_main64x64.py            # Evaluation script (64x64)
â”œâ”€â”€ Utils/
â”‚   â”œâ”€â”€ model.py                 # VAE and VQ-VAE architectures
â”‚   â”œâ”€â”€ train.py                 # VAETrainer & VQTrainer
â”‚   â”œâ”€â”€ data_preprocessor.py     # Data loader (128x128)
â”‚   â”œâ”€â”€ data_preprocessor 64x64.py # Data loader (64x64)
â”‚   â”œâ”€â”€ data_inspector.py        # Corruption check and stats
â”œâ”€â”€ Datas/
â”‚   â”œâ”€â”€ Subclass            # Train data in here
â”‚   â”œâ”€â”€ test                # Test data in here
â”œâ”€â”€ vae_model.pt                 # VAE (20 epochs)
â”œâ”€â”€ vae_model_50.pt              # VAE (50 epochs)
â”œâ”€â”€ vqvae_model.pt               # VQ-VAE 64x64 (20 epochs)
â”œâ”€â”€ vqvae_model_128x128.pt       # VQ-VAE 128x128 (50 epochs)

```

## ğŸ“ Model Checkpoints

* `vqvae_model.pt`: 64x64 VQ-VAE, trained 20 epochs
* `vqvae_model_128x128.pt`: 128x128 VQ-VAE, trained 50 epochs (recommended)

---

### 1. Prepare Your Dataset

Place your facial images in a subfolder inside `Datas/`, for example:

```
Datas/
â””â”€â”€ train/
    â”œâ”€â”€ image1.jpg
    â””â”€â”€ image2.png
```

> The folder name doesn't matter. PyTorch treats each subfolder as a class (unused here).

---

### 2. Train a VQ-VAE Model

You can choose between **64x64** and **128x128** training resolutions:

#### ğŸŸ¢ 64x64 Version:

Make sure to use `data_preprocessor 64x64.py`:

```bash
python main.py  # saves model to vqvae_model.pt
```

#### ğŸ”µ 128x128 Version:

Uses default `data_preprocessor.py`:

```bash
python main.py  # saves model to vqvae_model_128x128.pt
```

Training outputs include:

* Reconstruction + vector quantization loss
* Progress bar with loss metrics
* Saved model weights in `.pt` format

---

## ğŸ” Evaluate & Visualize

### Test script (128x128):

```bash
python test_main.py
```

### Test script (64x64):

```bash
python test_main64x64.py
```

Each script:

* Loads trained VQ-VAE
* Reconstructs a test image from `Datas/test/test1.png`
* Saves result to `Datas/test/reconstructed_vqvae.jpg`
* Displays compression rate and original vs reconstructed side-by-side

---

## ğŸ§± Code Modules

### `main.py`

Loads `DataPreprocessor`, `VQVAE`, and `VQTrainer` to run full training loop.

### `test_main*.py`

Evaluates model on test image, saves compressed result, prints stats.

### `Utils/model.py`

Defines:

* `VQVAE`: encoder â†’ quantizer â†’ decoder
* `VectorQuantizer`: nearest embedding codebook lookup

### `Utils/train.py`

* `VQTrainer`: training logic using MSE + codebook commitment loss

### `Utils/data_preprocessor*.py`

Image loaders for different resolutions. Includes:

* RGB conversion
* Resize pipeline

### `Utils/data_inspector.py`

Checks for corrupted or unreadable images and prints dataset info.

---

## ğŸ“ Notes

* Input resolution must match training config (64x64 or 128x128)
* Output images saved as JPEG (quality=85)
* Embedding vector size, codebook entries, and commitment cost can be tuned

---

## â­ï¸ Support

If you find this useful, feel free to â­ the repo and check out the full project series!

---

MIT License | Made with â¤ï¸ by ErtuÄŸrul Mutlu
